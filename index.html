<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Web Experimental SLAM v2</title>

<script src="https://cdn.jsdelivr.net/npm/three@0.160.0/build/three.min.js"></script>
<script src="https://docs.opencv.org/4.8.0/opencv.js"></script>

<style>
body { margin:0; overflow:hidden; }
video { display:none; }
canvas { position:absolute; top:0; left:0; }
</style>
</head>
<body>

<video id="video" autoplay playsinline></video>
<canvas id="three"></canvas>
<canvas id="cv" style="display:none;"></canvas>

<script>

let video = document.getElementById("video");
let threeCanvas = document.getElementById("three");
let cvCanvas = document.getElementById("cv");

let scene, camera, renderer, cube;

let prevGray = null;
let prevPoints = null;

let camX = 0;
let camZ = 0;

// เปิดกล้อง
navigator.mediaDevices.getUserMedia({
  video: { facingMode: "environment" }
}).then(stream => {
  video.srcObject = stream;
  video.play();
});

// THREE SETUP
function initThree() {

  renderer = new THREE.WebGLRenderer({
    canvas: threeCanvas,
    alpha:true
  });

  renderer.setSize(window.innerWidth, window.innerHeight);

  scene = new THREE.Scene();

  camera = new THREE.PerspectiveCamera(
    60,
    window.innerWidth/window.innerHeight,
    0.01,
    1000
  );

  camera.position.set(0,0,0);

  const geometry = new THREE.BoxGeometry(0.5,0.5,0.5);
  const material = new THREE.MeshNormalMaterial();
  cube = new THREE.Mesh(geometry, material);
  cube.position.set(0,0,-3);

  scene.add(cube);

  animate();
}

function animate(){
  requestAnimationFrame(animate);
  renderer.render(scene,camera);
}

// Feature tracking
function processFrame(){

  if (!video.videoWidth) {
    requestAnimationFrame(processFrame);
    return;
  }

  cvCanvas.width = video.videoWidth;
  cvCanvas.height = video.videoHeight;

  let ctx = cvCanvas.getContext("2d");
  ctx.drawImage(video,0,0);

  let src = cv.imread(cvCanvas);
  let gray = new cv.Mat();
  cv.cvtColor(src,gray,cv.COLOR_RGBA2GRAY);

  if (prevGray === null){

    prevGray = gray.clone();

    prevPoints = new cv.Mat();
    cv.goodFeaturesToTrack(
      gray,
      prevPoints,
      200,
      0.01,
      10
    );

  } else {

    let nextPoints = new cv.Mat();
    let status = new cv.Mat();
    let err = new cv.Mat();

    cv.calcOpticalFlowPyrLK(
      prevGray,
      gray,
      prevPoints,
      nextPoints,
      status,
      err
    );

    let dx = 0;
    let dy = 0;
    let count = 0;

    for (let i=0;i<status.rows;i++){
      if (status.data[i]===1){
        dx += nextPoints.data32F[i*2] - prevPoints.data32F[i*2];
        dy += nextPoints.data32F[i*2+1] - prevPoints.data32F[i*2+1];
        count++;
      }
    }

    if (count>0){
      dx/=count;
      dy/=count;

      // แปลง motion เป็น camera movement
      camX -= dx * 0.001;
      camZ += dy * 0.002;

      camera.position.x = camX;
      camera.position.z = camZ;
    }

    prevGray.delete();
    prevPoints.delete();

    prevGray = gray.clone();
    prevPoints = nextPoints.clone();

    nextPoints.delete();
    status.delete();
    err.delete();
  }

  src.delete();

  requestAnimationFrame(processFrame);
}

// รอ OpenCV
cv['onRuntimeInitialized'] = ()=>{
  initThree();
  processFrame();
};

</script>
</body>
</html>
